{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DQN Training in Maze Environment (Hyperparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import itertools\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": [5e-5, 1e-5, 1e-4],\n",
    "    \"gamma\": [0.96,0.96, 0.98],\n",
    "    \"epsilon_decay_episodes\": [0.85, 0.9, 0.95],\n",
    "    \"batch_size\": [128, 256],\n",
    "    \"replay_buffer_size\": [10000],\n",
    "    \"episodes\": [8000, 10000],\n",
    "    \"hidden_layers\": [\n",
    "        [64, 64, 64], \n",
    "        [128, 128, 128], \n",
    "        [128, 128], \n",
    "    ],\n",
    "    \"activation\": [\"elu\"]\n",
    "}\n",
    "\n",
    "def generate_param_combinations(grid):\n",
    "    keys = list(grid.keys())\n",
    "    values = list(grid.values())\n",
    "    for combination in itertools.product(*values):\n",
    "        yield dict(zip(keys, combination))\n",
    "\n",
    "shared_params = {\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"replay_buffer_size\": 8000,\n",
    "    \"steps_per_episode\": 3000,\n",
    "    \"target_update_freq\": 500,\n",
    "    \"random_seed\": 42,\n",
    "    \"maze_width\": 10,\n",
    "    \"maze_height\": 10,\n",
    "    \"maze_imperfections\": 5,\n",
    "    \"input_dim\": 19,\n",
    "    \"n_outputs\": 4,\n",
    "    \"log_every\": 11000,\n",
    "    \"q_log_every\": 11000,\n",
    "    \"plot_every\": 11000,\n",
    "    \"train_after\": 0.33333\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "maze_width = 16\n",
    "maze_height = 16\n",
    "input_shape = 19\n",
    "n_outputs = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "\n",
    "env = maze_library.init_environment_python(maze_width, maze_height, 5)\n",
    "maze_library.create_kruzkals_maze(env)\n",
    "maze_library.make_maze_imperfect(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers=[64, 64, 64], activation=\"elu\"):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            self.model.add_module(f\"fc{i}\", nn.Linear(prev_size, hidden_size))\n",
    "            self.model.add_module(f\"act{i}\", activation_map[activation]())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.model.add_module(\"output\", nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def create_model(hyperprm):\n",
    "    model = DQN(input_shape, n_outputs, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"])\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=5e-5)\n",
    "    loss_fn = nn.HuberLoss()\n",
    "\n",
    "    return model, optimizer, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy and Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_outputs - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())\n",
    "\n",
    "def sample_experiences(batch_size, replay_buffer):\n",
    "    indices = np.random.choice(len(replay_buffer), min(batch_size, len(replay_buffer)), replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    return zip(*batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def play_one_step(env, state, epsilon,model, replay_buffer, reward_log):\n",
    "    action = epsilon_greedy_policy(state, epsilon, model)\n",
    "    action_obj = maze_library.create_action(action, 0)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    reward_log.append(reward)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, optimizer, loss_fn, hyperprm, replay_buffer, q_value_log):\n",
    "    states, actions, rewards, next_states, dones, truncateds = sample_experiences(hyperprm[\"batch_size\"], replay_buffer)\n",
    "\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "    dones = torch.tensor(np.array(dones), dtype=torch.float32)\n",
    "    truncateds = torch.tensor(np.array(truncateds), dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = model(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "        target_q = rewards + (1 - terminal) * hyperprm[\"gamma\"] * max_next_q\n",
    "\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    q_value_log.extend(selected_q.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(q_value_log):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(q_value_log)\n",
    "    plt.title(\"Q-Values Over Time\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Q-Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(hyperprm, run_id):\n",
    "    all_episode_rewards  = []\n",
    "    reward_log = []\n",
    "    q_value_log = []\n",
    "    best_weights = None\n",
    "    highest_reward = -float(\"inf\")\n",
    "    model, optimizer, loss_fn = create_model(hyperprm)\n",
    "    replay_buffer = deque(maxlen=hyperprm[\"replay_buffer_size\"])\n",
    "    print(f\"Run {run_id}, Episodes: {hyperprm['episodes']}\")\n",
    "\n",
    "    for episode in range(hyperprm[\"episodes\"]):\n",
    "        obs, reward, done, truncated = env.reset()\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        for step in range(hyperprm[\"steps_per_episode\"]):\n",
    "            eps_decay = episode / (hyperprm[\"episodes\"] * hyperprm[\"epsilon_decay_episodes\"])\n",
    "            epsilon = max(hyperprm[\"epsilon_start\"] - eps_decay, hyperprm[\"epsilon_min\"])\n",
    "            obs, reward, done, truncated = play_one_step(env, obs, epsilon,model, replay_buffer, reward_log)\n",
    "            cumulative_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        if episode > (hyperprm[\"episodes\"] * hyperprm[\"train_after\"]):\n",
    "            training_step(model, optimizer, loss_fn, hyperprm, replay_buffer, q_value_log)\n",
    "        \n",
    "        if cumulative_reward > highest_reward:\n",
    "            highest_reward = cumulative_reward\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        if episode % hyperprm[\"log_every\"] == 0 and episode != 0:\n",
    "            print(f\"Run {run_id}, Reward: {np.average(all_episode_rewards[-100:])}\")\n",
    "\n",
    "        # if episode % hyperprm[\"q_log_every\"] == 0 and episode > 0:\n",
    "\n",
    "        all_episode_rewards.append(cumulative_reward)\n",
    "    model.load_state_dict(best_weights)\n",
    "    \n",
    "    plot_q_values(q_value_log)\n",
    "\n",
    "    return {\n",
    "        \"params\": hyperprm,\n",
    "        \"rewards\": all_episode_rewards,\n",
    "        \"q_values\": q_value_log,\n",
    "        \"final_model\": model,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "combo_params = list(generate_param_combinations(hyperparams))\n",
    "print(f\"Totals combinations = {len(combo_params)}\")\n",
    "for i, combo in enumerate(combo_params):\n",
    "    merged_params = {**shared_params, **combo}\n",
    "    results.append(run_single_experiment(merged_params, run_id=i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changed_hyperparams(defaults, current):\n",
    "    return {\n",
    "        k: v for k, v in current.items()\n",
    "        if k not in defaults or defaults[k] != v\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = math.ceil(len(results) / cols)\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, rows * 4))\n",
    "\n",
    "if len(results) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# Step 1: Find the max absolute reward for symmetric y-axis\n",
    "max_reward = max(\n",
    "    max(abs(min(result[\"rewards\"])), abs(max(result[\"rewards\"])))\n",
    "    for result in results\n",
    ")\n",
    "\n",
    "# Round up to make the graph cleaner\n",
    "y_limit = math.ceil(max_reward)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ax = axs[i // cols][i % cols] if rows > 1 else axs[i % cols]\n",
    "\n",
    "    rewards = result[\"rewards\"]\n",
    "    changed = get_changed_hyperparams(shared_params, result[\"params\"])\n",
    "\n",
    "    ax.plot(rewards)\n",
    "    ax.set_title(f\"Run {i} - Changed Params:\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Step 2: Set a constant center of 0\n",
    "    ax.set_ylim(-y_limit, y_limit)\n",
    "\n",
    "    # Annotate changed hyperparameters\n",
    "    annotation = \"\\n\".join([f\"{k}: {v}\" for k, v in changed.items()])\n",
    "    ax.text(1.01, 0.5, annotation, transform=ax.transAxes,\n",
    "            fontsize=9, verticalalignment='center',\n",
    "            bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, rows * cols):\n",
    "    fig.delaxes(axs[j // cols][j % cols] if rows > 1 else axs[j % cols])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"comparison_plot2.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_data = []\n",
    "for i, result in enumerate(results):\n",
    "    param_row = {**result[\"params\"]}\n",
    "    param_row[\"run_id\"] = i\n",
    "    param_row[\"mean_reward\"] = sum(result[\"rewards\"]) / len(result[\"rewards\"])\n",
    "    param_row[\"max_reward\"] = max(result[\"rewards\"])\n",
    "    param_row[\"min_reward\"] = min(result[\"rewards\"])\n",
    "    csv_data.append(param_row)\n",
    "\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv(\"results_log2.csv\", index=False)\n",
    "print(\"Saved results to results_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.hist(reward_log, bins=10, edgecolor='black')\n",
    "# plt.title(\"Reward Distribution\")\n",
    "# plt.xlabel(\"Reward\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
