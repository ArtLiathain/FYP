{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DQN Training in Maze Environment (Hyperparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "from utils import plot_q_histogram, plot_episode_returns, PrioritizedReplayBuffer, generate_param_combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": [1e-4],\n",
    "    \"gamma\": [0.96],\n",
    "    \"epsilon_decay_episodes\": [ 0.9],\n",
    "    \"batch_size\": [64],\n",
    "    \"replay_buffer_size\": [10000],\n",
    "    \"episodes\": [2000, 3000, 4000],\n",
    "    \"hidden_layers\": [\n",
    "        [64, 64, 64]\n",
    "    ],\n",
    "    \"activation\": [\"relu\",\"elu\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "shared_params = {\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"steps_per_episode\": 1000,\n",
    "    \"target_update_freq\": 500,\n",
    "    \"random_seed\": 42,\n",
    "    \"maze_width\": 7,\n",
    "    \"maze_height\": 7,\n",
    "    \"log_every\": 100,\n",
    "    \"q_log_every\": 2000,\n",
    "    \"plot_every\": 1100,\n",
    "    \"train_after\": 0.2,\n",
    "    \"mini_explore_runs_per_episode\":3,\n",
    "    \"mini_exploit_runs_per_episode\":2,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "env = maze_library.init_environment(shared_params[\"maze_width\"], shared_params[\"maze_height\"], mini_exploit_runs_per_episode=shared_params[\"mini_exploit_runs_per_episode\"], mini_explore_runs_per_episode=shared_params[\"mini_explore_runs_per_episode\"])\n",
    "maze_library.make_maze_imperfect(env)\n",
    "\n",
    "input_shape = env.input_shape()\n",
    "n_outputs = env.output_shape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "class DQNWithCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device, hidden_layers=[64, 64, 64], activation=\"elu\"):\n",
    "        super(DQNWithCNN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Constants\n",
    "        self.visual_feature_size = 175  # 5x5x7\n",
    "        self.visual_channels = 7\n",
    "        self.grid_size = 5\n",
    "\n",
    "        # Compute size of non-visual input\n",
    "        self.non_visual_input_size = input_size - self.visual_feature_size\n",
    "\n",
    "        # CNN for visual features (input shape: [batch, 7, 5, 5])\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.visual_channels, 32, kernel_size=3, padding=1),  # output: [batch, 32, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # output: [batch, 64, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.MaxPool2d(2),  # output: [batch, 64, 2, 2]\n",
    "        )\n",
    "\n",
    "        # Output from CNN flattened\n",
    "        cnn_output_size = 64 * 2 * 2\n",
    "\n",
    "        # FC layers for non-visual part\n",
    "        fc_input_size = cnn_output_size + self.non_visual_input_size\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        prev_size = fc_input_size\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            self.fc_layers.add_module(f\"fc{i}\", nn.Linear(prev_size, hidden_size))\n",
    "            self.fc_layers.add_module(f\"act{i}\", activation_map[activation]())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_layers.add_module(\"output\", nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into visual and non-visual components\n",
    "        visual_flat = x[:, -self.visual_feature_size:]  # Last 175 features\n",
    "        non_visual = x[:, :-self.visual_feature_size]   # Remaining features\n",
    "\n",
    "        # Reshape visual input to [batch, 7, 5, 5]\n",
    "        visual = visual_flat.view(-1, self.visual_channels, self.grid_size, self.grid_size)\n",
    "\n",
    "        # Process visual features through CNN\n",
    "        visual_out = self.cnn(visual)\n",
    "        visual_out = visual_out.view(visual_out.size(0), -1)  # Flatten\n",
    "\n",
    "        # Concatenate visual and non-visual features\n",
    "        x_combined = torch.cat((non_visual, visual_out), dim=1)\n",
    "\n",
    "        # Forward through FC layers\n",
    "        return self.fc_layers(x_combined)\n",
    "\n",
    "\n",
    "def create_model(hyperprm):\n",
    "    model = DQNWithCNN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"]).to(DEVICE)\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=hyperprm[\"learning_rate\"])\n",
    "    loss_fn = nn.HuberLoss()\n",
    "\n",
    "    return model, optimizer, loss_fn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy and Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_outputs - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play_one_step(env, state, epsilon,model, replay_buffer, run):\n",
    "    action = epsilon_greedy_policy(state, epsilon, model)\n",
    "    action_obj = maze_library.create_action(action, run)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    replay_buffer.append(state, action, reward, next_state, done, truncated)\n",
    "    return next_state, reward, done, truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses):\n",
    "    batch = replay_buffer.sample(hyperprm[\"batch_size\"])    \n",
    "    states = torch.tensor(batch[\"state\"], dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(batch[\"action\"], dtype=torch.int64, device=DEVICE)\n",
    "    rewards = torch.tensor(batch[\"reward\"], dtype=torch.float32, device=DEVICE)\n",
    "    next_states = torch.tensor(batch[\"next_state\"], dtype=torch.float32, device=DEVICE)\n",
    "    dones = torch.tensor(batch[\"done\"], dtype=torch.float32, device=DEVICE)\n",
    "    truncateds = torch.tensor(batch[\"truncated\"], dtype=torch.float32, device=DEVICE)\n",
    "    is_weights = torch.tensor(batch[\"weights\"], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = model(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "        target_q = rewards + (1 - terminal) * hyperprm[\"gamma\"] * max_next_q\n",
    "\n",
    "    # Compute current Q-values\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "    # TD error and loss\n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "    loss = (is_weights * loss.pow(2)).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update priorities\n",
    "    td_errors = selected_q - target_q\n",
    "    new_priorities = td_errors.detach().abs().cpu().numpy() + 1e-5\n",
    "    replay_buffer.update_priorities(batch[\"indices\"], new_priorities)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    q_value_log.extend(selected_q.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(hyperprm,timestamp, run_id):\n",
    "    all_episode_explore_rewards  = []\n",
    "    all_episode_exploit_rewards  = []\n",
    "    losses = []\n",
    "    report_cards = []\n",
    "    q_value_log = []\n",
    "    model, optimizer, loss_fn = create_model(hyperprm)\n",
    "    replay_buffer = PrioritizedReplayBuffer(capacity=hyperprm[\"replay_buffer_size\"], state_shape=(input_shape,))\n",
    "    \n",
    "    print(f\"Run {run_id}, Episodes: {hyperprm['episodes']}\")\n",
    "    folder_name = f\"../mazeLogs/{timestamp}DoubleDQNMaze/Run{run_id}\"\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "    filecount = 0\n",
    "    \n",
    "    \n",
    "    for episode in range(hyperprm[\"episodes\"]):\n",
    "        obs = env.reset_and_regenerate()\n",
    "        current_episode_explore_rewards = []\n",
    "        current_episode_exploit_rewards = []\n",
    "        eps_decay = episode / (hyperprm[\"episodes\"] * hyperprm[\"epsilon_decay_episodes\"])\n",
    "        epsilon = max(hyperprm[\"epsilon_start\"] - eps_decay, hyperprm[\"epsilon_min\"])\n",
    "        for mini_episode in range((hyperprm[\"mini_explore_runs_per_episode\"] + hyperprm[\"mini_exploit_runs_per_episode\"])):\n",
    "            obs = env.smart_reset(mini_episode)\n",
    "            for _ in range(hyperprm[\"steps_per_episode\"]):\n",
    "                obs, reward, done, truncated = play_one_step(env, obs, epsilon,model, replay_buffer, mini_episode)\n",
    "                if mini_episode < hyperprm[\"mini_explore_runs_per_episode\"]:\n",
    "                    current_episode_explore_rewards.append(reward)\n",
    "                else:\n",
    "                    current_episode_exploit_rewards.append(reward)\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "                \n",
    "\n",
    "        score = maze_library.get_score(env)\n",
    "        all_episode_explore_rewards.append(np.sum(np.array(current_episode_explore_rewards)))\n",
    "        all_episode_exploit_rewards.append(np.sum(np.array(current_episode_exploit_rewards)))\n",
    "                \n",
    "        if episode > (hyperprm[\"episodes\"] * hyperprm[\"train_after\"]):\n",
    "            training_step(model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses)     \n",
    "            \n",
    "        if episode % hyperprm[\"log_every\"] == 0 and episode != 0:\n",
    "            print(f\"\"\"Run {run_id}, Total steps {score.total_steps},Average Solve Score {float(f\"{score.average_run_score/ score.dijkstra_shortest_path_score:.2g}\") } filecount: {filecount} \n",
    "                  Explore Rewards sum: {float(f\"{np.sum(np.array(current_episode_explore_rewards)):.2g}\")} Exploit Rewards sum: {float(f\"{np.sum(np.array(current_episode_exploit_rewards)):.2g}\")} \n",
    "                  Success Rate in solving {score.success_rate_in_exploitation} Episode: {episode}\"\"\")\n",
    "            with open(f'{folder_name}/doubledqn{filecount}.json', 'w') as file:\n",
    "                file.write(env.to_json_python())\n",
    "            filecount+=1\n",
    "    \n",
    "    plot_q_histogram(q_value_log)\n",
    "\n",
    "    return {\n",
    "        \"params\": hyperprm,\n",
    "        \"explore_rewards\": all_episode_explore_rewards,\n",
    "        \"exploit_rewards\": all_episode_exploit_rewards,\n",
    "        \"score\" : report_cards,\n",
    "        \"q_values\": q_value_log,\n",
    "        \"final_model\": model,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "combo_params = list(generate_param_combinations(hyperparams))\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m_%H-%M\")\n",
    "run_count = 0\n",
    "print(f\"Totals combinations = {len(combo_params)}\")\n",
    "for  combo in combo_params:\n",
    "    for j in range(2):\n",
    "        merged_params = {**shared_params, **combo}\n",
    "        results.append(run_single_experiment(merged_params,timestamp, run_id=run_count))\n",
    "        run_count += 1\n",
    "        plot_episode_returns(results[-1][\"explore_rewards\"], results[-1][\"exploit_rewards\"], 10)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
