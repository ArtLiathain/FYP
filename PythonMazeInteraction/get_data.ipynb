{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_experiment_results_with_pickle, plot_episode_returns, plot_report_card_metric, calculate_mean_std_of_metric_per_algorithm\n",
    "import maze_library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_experiment_results_with_pickle(\"../mazeLogs/14-05_13-07PPO/Run0/experiment_results.pkl\")\n",
    "training_report_parsed = [maze_library.report_card_from_json(rc) for rc in results[\"training_report\"]]\n",
    "\n",
    "\n",
    "testing_maze_report_parsed = {\n",
    "    k: [maze_library.report_card_from_json(rc) for rc in v]\n",
    "    for k, v in results[\"testing_maze_report_cards\"].items()\n",
    "}\n",
    "\n",
    "plot_episode_returns(results[\"training_explore_rewards\"], results[\"training_exploit_rewards\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[\"params\"][\"gen_algorithm\"])\n",
    "plot_report_card_metric(testing_maze_report_parsed, \"total_percentage_explored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bbd12",
   "metadata": {},
   "source": [
    "## Calculate Required Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e40f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = list(testing_maze_report_parsed.keys())\n",
    "agent_score = []\n",
    "\n",
    "# Calculate average_run_score per algorithm (ignoring zeros)\n",
    "for algo in algorithms:\n",
    "    values = [getattr(report, \"average_run_score\") for report in testing_maze_report_parsed[algo] if getattr(report, \"average_run_score\") != 0]\n",
    "    avg_score = sum(values) / len(values) if values else 0.0\n",
    "    agent_score.append(avg_score)\n",
    "\n",
    "# Get other metrics\n",
    "algorithms, dijkstra_score, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"dijkstra_shortest_path_score\")\n",
    "_, success_rate, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"success_rate_in_exploitation\")\n",
    "\n",
    "# Print all variables separately\n",
    "print(\"\\n=== Experiment Parameters ===\")\n",
    "print(f\"Generation Algorithm: {results['params']['gen_algorithm']}\")\n",
    "compund_scores = []\n",
    "print(\"\\n=== Exploitation Metrics per Algorithm ===\")\n",
    "for i, algo in enumerate(algorithms[:5]):\n",
    "    # print(f\"  Agent Score: {agent_score[i]:.4f}\")\n",
    "    # print(f\"  Dijkstra Score: {dijkstra_score[i]:.4f}\")\n",
    "    # print(f\"  Success Rate: {success_rate[i]:.4f}\")\n",
    "    if agent_score[i] != 0:\n",
    "        compound_score = (dijkstra_score[i] / agent_score[i]) * success_rate[i]\n",
    "        # print(f\"  Compound Score: {compound_score:.4f}\")\n",
    "        print(\"\\\\hline\")\n",
    "        print(f\"{algo} & {dijkstra_score[i]:.2f} & {agent_score[i]:.2f} & {success_rate[i]:.2f} & {compound_score:.2f} \\\\\\\\\")\n",
    "        compund_scores.append(compound_score)\n",
    "    else:\n",
    "        print(\"  Compound Score: Undefined (agent_score is 0)\")\n",
    "total_avg_agent_score = sum(agent_score) / len(agent_score) if agent_score else 0.0\n",
    "total_avg_dijk_score = sum(dijkstra_score) / len(dijkstra_score) if dijkstra_score else 0.0\n",
    "success_rate_score = sum(success_rate) / len(success_rate) if success_rate else 0.0\n",
    "compound_score_avg = sum(compund_scores) / len(compund_scores) if compund_scores else 0.0\n",
    "print(\"\\\\hline\")\n",
    "\n",
    "print(f\"\\\\textbf{{Overall Average}} & \\\\textbf{{{total_avg_dijk_score:.2f}}} & \\\\textbf{{{total_avg_agent_score:.2f}}} & \\\\textbf{{{success_rate_score:.2f}}} & \\\\textbf{{{compound_score_avg:.2f}}} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = list(testing_maze_report_parsed.keys())\n",
    "\n",
    "# Get other metrics\n",
    "_, exploration_percentage, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"total_percentage_explored\")\n",
    "_, local_tile_revisits, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"average_visits\")\n",
    "_, local_exploration_percentage, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"percentage_visited\")\n",
    "\n",
    "# Print experiment parameters\n",
    "print(\"\\n=== Experiment Parameters ===\")\n",
    "print(f\"Generation Algorithm: {results['params']['gen_algorithm']}\")\n",
    "compound_scores = []\n",
    "# Print metrics per algorithm in compact format\n",
    "print(\"\\n=== Exploration Metrics per Algorithm ===\")\n",
    "for i, algo in enumerate(algorithms[:5]):\n",
    "    print(\"\\\\hline\")\n",
    "    \n",
    "    compound_score = ((local_exploration_percentage[i] * 100) / local_tile_revisits[i])\n",
    "    compound_scores.append(compound_score)\n",
    "    print(f\"{algo} & {exploration_percentage[i]:.2f} & {local_tile_revisits[i]:.2f} & {local_exploration_percentage[i]:.2f} & {compound_score:.2f} \\\\\\\\\")\n",
    "avg_exploration_percentage = sum(exploration_percentage) / len(exploration_percentage) if exploration_percentage else 0.0\n",
    "avg_local_tile_revisits = sum(local_tile_revisits) / len(local_tile_revisits) if local_tile_revisits else 0.0\n",
    "avg_local_exploration_percentage = sum(local_exploration_percentage) / len(local_exploration_percentage) if local_exploration_percentage else 0.0\n",
    "compound_score_avg = sum(compound_scores) / len(compound_scores) if compound_scores else 0.0\n",
    "print(\"\\\\hline\")\n",
    "print(f\"\\\\textbf{{Overall Average}} & \\\\textbf{{{avg_exploration_percentage:.2f}\\\\%}} & \\\\textbf{{{avg_local_tile_revisits:.2f}}} & \\\\textbf{{{avg_local_exploration_percentage:.2f}\\\\%}} & \\\\textbf{{{compound_score_avg:.2f}}} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = list(testing_maze_report_parsed.keys())\n",
    "\n",
    "# Get other metrics\n",
    "_, walls_hit, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"walls_hit\")\n",
    "_, full_turns_done, _ = calculate_mean_std_of_metric_per_algorithm(testing_maze_report_parsed, \"full_turns_done\")\n",
    "\n",
    "# Print experiment parameters\n",
    "print(\"\\n=== Experiment Parameters ===\")\n",
    "print(f\"Generation Algorithm: {results['params']['gen_algorithm']}\")\n",
    "\n",
    "# Print metrics per algorithm in compact format\n",
    "print(\"\\n=== Inefficiency Metrics per Algorithm ===\")\n",
    "for i, algo in enumerate(algorithms[:5]):\n",
    "    compound_score = walls_hit[i] + full_turns_done[i]\n",
    "    print(\"\\\\hline\")\n",
    "    \n",
    "    print(f\"{algo} & {walls_hit[i]:.2f} & {full_turns_done[i]:.2f} & {compound_score:.2f} \\\\\\\\\")\n",
    "avg_walls_hit = sum(walls_hit) / len(walls_hit) if walls_hit else 0.0\n",
    "avg_full_turns_done = sum(full_turns_done) / len(full_turns_done) if full_turns_done else 0.0\n",
    "print(\"\\\\hline\")\n",
    "print(f\"\\\\textbf{{Overall Average}} & \\\\textbf{{{avg_walls_hit:.2f}}} & \\\\textbf{{{avg_full_turns_done:.2f}}} & \\\\textbf{{{(avg_walls_hit + avg_full_turns_done):.2f}}} \\\\\\\\\")\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a9a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
