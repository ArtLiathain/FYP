{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DQN Training in Maze Environment (Hyperparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "from utils import PrioritizedReplayBuffer, moving_average, plot_episode_returns, generate_param_combinations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"learning_rate\":[1e-5],\n",
    "    \"gamma\":[ 0.97],\n",
    "    \"epsilon_decay_episodes\":[ 0.9],\n",
    "    \"batch_size\":[ 64],\n",
    "    \"replay_buffer_size\":[ 10000],\n",
    "    \"episodes\":[ 4000],\n",
    "    \"hidden_layers\":[ \n",
    "        [512, 256, 128] \n",
    "    ],\n",
    "    \"activation\":[ \"relu\"],\n",
    "    \"target_dqn_update\" :[ 10],\n",
    "    \n",
    "}\n",
    "shared_params = {\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"steps_per_episode\": 1000,\n",
    "    \"target_update_freq\": 500,\n",
    "    \"random_seed\": 42,\n",
    "    \"maze_width\": 7,\n",
    "    \"maze_height\": 7,\n",
    "    \"log_every\": 100,\n",
    "    \"q_log_every\": 2000,\n",
    "    \"plot_every\": 1100,\n",
    "    \"train_after\": 0.2,\n",
    "    \"mini_explore_runs_per_episode\":3,\n",
    "    \"mini_exploit_runs_per_episode\":2,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "env = maze_library.init_environment(shared_params[\"maze_width\"], shared_params[\"maze_height\"], mini_exploit_runs_per_episode=shared_params[\"mini_exploit_runs_per_episode\"], mini_explore_runs_per_episode=shared_params[\"mini_explore_runs_per_episode\"])\n",
    "maze_library.make_maze_imperfect(env)\n",
    "\n",
    "input_shape = env.input_shape()\n",
    "n_outputs = env.output_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "class DQNWithCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device, hidden_layers=[64, 64, 64], activation=\"elu\"):\n",
    "        super(DQNWithCNN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Constants\n",
    "        self.visual_feature_size = 175  # 5x5x7\n",
    "        self.visual_channels = 7\n",
    "        self.grid_size = 5\n",
    "\n",
    "        # Compute size of non-visual input\n",
    "        self.non_visual_input_size = input_size - self.visual_feature_size\n",
    "\n",
    "        # CNN for visual features (input shape: [batch, 7, 5, 5])\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.visual_channels, 32, kernel_size=3, padding=1),  # output: [batch, 32, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # output: [batch, 64, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.MaxPool2d(2),  # output: [batch, 64, 2, 2]\n",
    "        )\n",
    "\n",
    "        # Output from CNN flattened\n",
    "        cnn_output_size = 64 * 2 * 2\n",
    "\n",
    "        # FC layers for non-visual part\n",
    "        fc_input_size = cnn_output_size + self.non_visual_input_size\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        prev_size = fc_input_size\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            self.fc_layers.add_module(f\"fc{i}\", nn.Linear(prev_size, hidden_size))\n",
    "            self.fc_layers.add_module(f\"act{i}\", activation_map[activation]())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_layers.add_module(\"output\", nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into visual and non-visual components\n",
    "        visual_flat = x[:, -self.visual_feature_size:]  # Last 175 features\n",
    "        non_visual = x[:, :-self.visual_feature_size]   # Remaining features\n",
    "\n",
    "        # Reshape visual input to [batch, 7, 5, 5]\n",
    "        visual = visual_flat.view(-1, self.visual_channels, self.grid_size, self.grid_size)\n",
    "\n",
    "        # Process visual features through CNN\n",
    "        visual_out = self.cnn(visual)\n",
    "        visual_out = visual_out.view(visual_out.size(0), -1)  # Flatten\n",
    "\n",
    "        # Concatenate visual and non-visual features\n",
    "        x_combined = torch.cat((non_visual, visual_out), dim=1)\n",
    "\n",
    "        # Forward through FC layers\n",
    "        return self.fc_layers(x_combined)\n",
    "\n",
    "\n",
    "def create_double_dqn_model(hyperprm):\n",
    "    model = DQNWithCNN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"]).to(DEVICE)\n",
    "    target_model = DQNWithCNN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"]).to(DEVICE)\n",
    "    target_model.load_state_dict(model.state_dict())  \n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=hyperprm[\"learning_rate\"])  \n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    return model, target_model, optimizer, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy and Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_outputs - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon,model, replay_buffer, run):\n",
    "    action = epsilon_greedy_policy(state, epsilon, model)\n",
    "    action_obj = maze_library.create_action(action, run)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    replay_buffer.append(state, action, reward, next_state, done, truncated)\n",
    "    return next_state, reward, done, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses):\n",
    "    batch = replay_buffer.sample(hyperprm[\"batch_size\"])    \n",
    "    states = torch.tensor(batch[\"state\"], dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(batch[\"action\"], dtype=torch.int64, device=DEVICE)\n",
    "    rewards = torch.tensor(batch[\"reward\"], dtype=torch.float32, device=DEVICE)\n",
    "    next_states = torch.tensor(batch[\"next_state\"], dtype=torch.float32, device=DEVICE)\n",
    "    dones = torch.tensor(batch[\"done\"], dtype=torch.float32, device=DEVICE)\n",
    "    truncateds = torch.tensor(batch[\"truncated\"], dtype=torch.float32, device=DEVICE)\n",
    "    is_weights = torch.tensor(batch[\"weights\"], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model(next_states)\n",
    "        next_actions = next_q_values.argmax(dim=1)\n",
    "        target_q_values = target_model(next_states)\n",
    "        max_next_q = target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "        target_q = rewards + (1 - terminal) * hyperprm[\"gamma\"] * max_next_q\n",
    "\n",
    "    # Compute current Q-values\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "    # TD error and loss\n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "    loss = (is_weights * loss.pow(2)).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update priorities\n",
    "    td_errors = selected_q - target_q\n",
    "    new_priorities = td_errors.detach().abs().cpu().numpy() + 1e-5\n",
    "    replay_buffer.update_priorities(batch[\"indices\"], new_priorities)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    q_value_log.extend(selected_q.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_losses, plot_q_histogram\n",
    "\n",
    "\n",
    "def run_single_experiment(hyperprm, timestamp,run_id):\n",
    "    all_episode_explore_rewards  = []\n",
    "    all_episode_exploit_rewards  = []\n",
    "    losses = []\n",
    "    report_cards = []\n",
    "    q_value_log = []\n",
    "    filecount = 0\n",
    "    folder_name = f\"../mazeLogs/{timestamp}DoubleDQNMaze/Run{run_id}\"\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "\n",
    "    model,target_model, optimizer, loss_fn = create_double_dqn_model(hyperprm)\n",
    "    replay_buffer = PrioritizedReplayBuffer(capacity=hyperprm[\"replay_buffer_size\"], state_shape=(input_shape,))\n",
    "    print(f\"Run {run_id}, Episodes: {hyperprm['episodes']}\")\n",
    "    for episode in range(hyperprm[\"episodes\"]):\n",
    "\n",
    "        obs = env.reset_and_regenerate()\n",
    "        current_episode_explore_rewards = []\n",
    "        current_episode_exploit_rewards = []\n",
    "        eps_decay = episode / (hyperprm[\"episodes\"] * hyperprm[\"epsilon_decay_episodes\"])\n",
    "        epsilon = max(hyperprm[\"epsilon_start\"] - eps_decay, hyperprm[\"epsilon_min\"])\n",
    "        for mini_episode in range((hyperprm[\"mini_explore_runs_per_episode\"] + hyperprm[\"mini_exploit_runs_per_episode\"])):\n",
    "            obs = env.smart_reset(mini_episode)\n",
    "            for _ in range(hyperprm[\"steps_per_episode\"]):\n",
    "                obs, reward, done, truncated = play_one_step(env, obs, epsilon,model, replay_buffer, mini_episode)\n",
    "                if mini_episode < hyperprm[\"mini_explore_runs_per_episode\"]:\n",
    "                    current_episode_explore_rewards.append(reward)\n",
    "                else:\n",
    "                    current_episode_exploit_rewards.append(reward)\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "                \n",
    "\n",
    "        if episode > (hyperprm[\"episodes\"] * hyperprm[\"train_after\"]):\n",
    "            training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses)        \n",
    "        if episode % hyperprm[\"target_dqn_update\"] == 0 and episode > 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        \n",
    "        score = maze_library.get_score(env)\n",
    "        report_cards.append(score)\n",
    "        all_episode_explore_rewards.append(np.sum(np.array(current_episode_explore_rewards)))\n",
    "        all_episode_exploit_rewards.append(np.sum(np.array(current_episode_exploit_rewards)))\n",
    "        if episode % hyperprm[\"log_every\"] == 0 and episode != 0:\n",
    "            print(f\"\"\"Run {run_id}, Total steps {score.total_steps},Average Solve Score {float(f\"{score.average_run_score/ score.dijkstra_shortest_path_score:.2g}\") } filecount: {filecount} \n",
    "                  Explore Rewards sum: {float(f\"{np.sum(np.array(current_episode_explore_rewards)):.2g}\")} Exploit Rewards sum: {float(f\"{np.sum(np.array(current_episode_exploit_rewards)):.2g}\")} \n",
    "                  Success Rate in solving {score.success_rate_in_exploitation} Episode: {episode}\"\"\")\n",
    "            with open(f'{folder_name}/doubledqn{filecount}.json', 'w') as file:\n",
    "                file.write(env.to_json_python())\n",
    "            \n",
    "            filecount += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    plot_q_histogram(q_value_log)\n",
    "    plot_losses(losses)\n",
    "    \n",
    "    return {\n",
    "        \"params\": hyperprm,\n",
    "        \"explore_rewards\": all_episode_explore_rewards,\n",
    "        \"exploit_rewards\": all_episode_exploit_rewards,\n",
    "        \"score\" : report_cards,\n",
    "        \"q_values\": q_value_log,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "combo_params = list(generate_param_combinations(hyperparams))\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m_%H-%M\")\n",
    "run_count = 0\n",
    "print(f\"Totals combinations = {len(combo_params)}\")\n",
    "for i, combo in enumerate(combo_params):\n",
    "    for j in range(2):\n",
    "        merged_params = {**shared_params, **combo}\n",
    "        results.append(run_single_experiment(merged_params,timestamp, run_id=run_count))\n",
    "        run_count += 1\n",
    "        plot_episode_returns(results[-1][\"explore_rewards\"], results[-1][\"exploit_rewards\"], 10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_returns(results[-1][\"explore_rewards\"], results[-1][\"exploit_rewards\"], 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "steps_nested  = [r[\"score\"] for r in results]\n",
    "steps = [score.total_steps for sublist in steps_nested for score in sublist] \n",
    "\n",
    "\n",
    "# Smoothing window size\n",
    "window_size = 15  # You can adjust this window size for more or less smoothing\n",
    "\n",
    "# Determine the number of subplots (rows x columns)\n",
    "n_runs = len(steps_nested)\n",
    "cols = 3  # Number of columns\n",
    "rows = (n_runs // cols) + (n_runs % cols > 0)  # Calculate number of rows required\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "\n",
    "# Flatten axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each run in its own subplot\n",
    "for i, run_scores in enumerate(steps_nested):\n",
    "    steps = [s.total_steps for s in run_scores]\n",
    "    \n",
    "    # Apply moving average smoothing to the data\n",
    "    smoothed_steps = moving_average(steps, window_size)\n",
    "    \n",
    "    # Select the current axis for this subplot\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot the smoothed steps on the corresponding axis\n",
    "    ax.plot(smoothed_steps, label=f\"Run {i+1}\")\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Total Steps\")\n",
    "    ax.set_title(f\"Performance of Run {i+1}\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove any empty subplots if the number of runs isn't a perfect multiple of columns\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
