{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DQN Training in Maze Environment (Hyperparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import datetime\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from utils import PrioritizedReplayBuffer, generate_param_combinations, save_experiment_results_with_pickle, load_experiment_results_with_pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"learning_rate\":[5e-4],\n",
    "    \"gamma\":[ 0.97],\n",
    "    \"epsilon_decay_episodes\":[ 0.9],\n",
    "    \"batch_size\":[ 64],\n",
    "    \"replay_buffer_size\":[ 10000],\n",
    "    \"episodes\":[ 6000 ],\n",
    "    \"hidden_layers\":[ \n",
    "        [512, 256, 128],\n",
    "    ],\n",
    "    \"activation\":[\"relu\"],\n",
    "    \"target_dqn_update\" :[ 30],\n",
    "    \"gen_algorithm\": [\"BinaryTree\"],\n",
    "    \"maze_size\": [7],\n",
    "}\n",
    "shared_params = {\n",
    "    \"test_episodes\": 200,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"steps_per_episode\": 1000,\n",
    "    \"random_seed\": 42,\n",
    "    \"log_every\": 400,\n",
    "    \"q_log_every\": 2000,\n",
    "    \"plot_every\": 1100,\n",
    "    \"train_after\": 0.2,\n",
    "    \"mini_explore_runs_per_episode\":3,\n",
    "    \"mini_exploit_runs_per_episode\":2,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maze_library.maze_generation_algorithms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "class DQNWithCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device, hidden_layers=[64, 64, 64], activation=\"elu\"):\n",
    "        super(DQNWithCNN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Constants\n",
    "        self.visual_feature_size = 175  # 5x5x7\n",
    "        self.visual_channels = 7\n",
    "        self.grid_size = 5\n",
    "\n",
    "        # Compute size of non-visual input\n",
    "        self.non_visual_input_size = input_size - self.visual_feature_size\n",
    "\n",
    "        # CNN for visual features (input shape: [batch, 7, 5, 5])\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.visual_channels, 32, kernel_size=3, padding=1),  # output: [batch, 32, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # output: [batch, 64, 5, 5]\n",
    "            activation_map[activation](),\n",
    "            nn.MaxPool2d(2),  # output: [batch, 64, 2, 2]\n",
    "        )\n",
    "\n",
    "        # Output from CNN flattened\n",
    "        cnn_output_size = 64 * 2 * 2\n",
    "\n",
    "        # FC layers for non-visual part\n",
    "        fc_input_size = cnn_output_size + self.non_visual_input_size\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        prev_size = fc_input_size\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            self.fc_layers.add_module(f\"fc{i}\", nn.Linear(prev_size, hidden_size))\n",
    "            self.fc_layers.add_module(f\"act{i}\", activation_map[activation]())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_layers.add_module(\"output\", nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into visual and non-visual components\n",
    "        visual_flat = x[:, -self.visual_feature_size:]  # Last 175 features\n",
    "        non_visual = x[:, :-self.visual_feature_size]   # Remaining features\n",
    "\n",
    "        # Reshape visual input to [batch, 7, 5, 5]\n",
    "        visual = visual_flat.view(-1, self.visual_channels, self.grid_size, self.grid_size)\n",
    "\n",
    "        # Process visual features through CNN\n",
    "        visual_out = self.cnn(visual)\n",
    "        visual_out = visual_out.view(visual_out.size(0), -1)  # Flatten\n",
    "\n",
    "        # Concatenate visual and non-visual features\n",
    "        x_combined = torch.cat((non_visual, visual_out), dim=1)\n",
    "\n",
    "        # Forward through FC layers\n",
    "        return self.fc_layers(x_combined)\n",
    "\n",
    "\n",
    "def create_double_dqn_model(hyperprm, input_shape, n_outputs):\n",
    "    model = DQNWithCNN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"]).to(DEVICE)\n",
    "    target_model = DQNWithCNN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"]).to(DEVICE)\n",
    "    target_model.load_state_dict(model.state_dict())  \n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=hyperprm[\"learning_rate\"])  \n",
    "    loss_fn = nn.HuberLoss()\n",
    "\n",
    "    return model, target_model, optimizer, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy and Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon, model, n_outputs):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_outputs - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon,model, replay_buffer, run, n_outputs):\n",
    "    action = epsilon_greedy_policy(state, epsilon, model, n_outputs)\n",
    "    action_obj = maze_library.create_action(action, run)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    replay_buffer.append(state, action, reward, next_state, done, truncated)\n",
    "    return next_state, reward, done, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses):\n",
    "    batch = replay_buffer.sample(hyperprm[\"batch_size\"])    \n",
    "    states = torch.tensor(batch[\"state\"], dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(batch[\"action\"], dtype=torch.int64, device=DEVICE)\n",
    "    rewards = torch.tensor(batch[\"reward\"], dtype=torch.float32, device=DEVICE)\n",
    "    next_states = torch.tensor(batch[\"next_state\"], dtype=torch.float32, device=DEVICE)\n",
    "    dones = torch.tensor(batch[\"done\"], dtype=torch.float32, device=DEVICE)\n",
    "    truncateds = torch.tensor(batch[\"truncated\"], dtype=torch.float32, device=DEVICE)\n",
    "    is_weights = torch.tensor(batch[\"weights\"], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model(next_states)\n",
    "        next_actions = next_q_values.argmax(dim=1)\n",
    "        target_q_values = target_model(next_states)\n",
    "        max_next_q = target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "        target_q = rewards + (1 - terminal) * hyperprm[\"gamma\"] * max_next_q\n",
    "\n",
    "    # Compute current Q-values\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "    # TD error and loss\n",
    "    td_errors = selected_q - target_q  # TD error remains the same\n",
    "    loss = (is_weights * loss_fn(selected_q, target_q)).mean()\n",
    "    is_weights = is_weights / is_weights.max()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update priorities\n",
    "    new_priorities = td_errors.detach().abs().cpu().numpy() + 1e-5\n",
    "    replay_buffer.update_priorities(batch[\"indices\"], new_priorities)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    q_value_log.extend(selected_q.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_losses, plot_q_histogram\n",
    "\n",
    "\n",
    "def run_single_experiment(hyperprm, timestamp,run_id):\n",
    "    all_episode_explore_rewards  = []\n",
    "    all_episode_exploit_rewards  = []\n",
    "    losses = []\n",
    "    report_cards = []\n",
    "    q_value_log = []\n",
    "    maze_generation_algorithms = maze_library.maze_generation_algorithms()\n",
    "    filecount = 0\n",
    "    folder_name = f\"../mazeLogs/{timestamp}DoubleDQNMaze/Run{run_id}\"\n",
    "    env = maze_library.init_environment(hyperprm[\"maze_size\"], hyperprm[\"maze_size\"], \n",
    "                                    gen_algorithm=hyperprm[\"gen_algorithm\"],\n",
    "                                    mini_exploit_runs_per_episode=hyperprm[\"mini_exploit_runs_per_episode\"], \n",
    "                                    mini_explore_runs_per_episode=hyperprm[\"mini_explore_runs_per_episode\"])\n",
    "    input_shape = env.input_shape()\n",
    "    n_outputs = env.output_shape()\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "    model,target_model, optimizer, loss_fn = create_double_dqn_model(hyperprm, input_shape, n_outputs)\n",
    "    replay_buffer = PrioritizedReplayBuffer(capacity=hyperprm[\"replay_buffer_size\"], state_shape=(input_shape,))\n",
    "    print(f\"Run {run_id}, Episodes: {hyperprm['episodes']}\")\n",
    "    for episode in range(hyperprm[\"episodes\"]):\n",
    "        obs = env.reset_and_regenerate()\n",
    "        current_episode_explore_rewards = []\n",
    "        current_episode_exploit_rewards = []\n",
    "        eps_decay = episode / (hyperprm[\"episodes\"] * hyperprm[\"epsilon_decay_episodes\"])\n",
    "        epsilon = max(hyperprm[\"epsilon_start\"] - eps_decay, hyperprm[\"epsilon_min\"])\n",
    "        for mini_episode in range((hyperprm[\"mini_explore_runs_per_episode\"] + hyperprm[\"mini_exploit_runs_per_episode\"])):\n",
    "            obs = env.smart_reset(mini_episode)\n",
    "            for _ in range(hyperprm[\"steps_per_episode\"]):\n",
    "                obs, reward, done, truncated = play_one_step(env, obs, epsilon,model, replay_buffer, mini_episode, n_outputs)\n",
    "                if mini_episode < hyperprm[\"mini_explore_runs_per_episode\"]:\n",
    "                    current_episode_explore_rewards.append(reward)\n",
    "                else:\n",
    "                    current_episode_exploit_rewards.append(reward)\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "                \n",
    "\n",
    "        if episode > (hyperprm[\"episodes\"] * hyperprm[\"train_after\"]):\n",
    "            training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, losses)        \n",
    "        if episode % hyperprm[\"target_dqn_update\"] == 0 and episode > 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        \n",
    "        report_card = maze_library.get_score(env)\n",
    "        report_cards.append(report_card.to_json())\n",
    "        all_episode_explore_rewards.append(np.sum(np.array(current_episode_explore_rewards)))\n",
    "        all_episode_exploit_rewards.append(np.sum(np.array(current_episode_exploit_rewards)))\n",
    "        if episode % hyperprm[\"log_every\"] == 0 and episode != 0:\n",
    "            # print(f\"\"\"Run {run_id}, Total steps {report_card.total_steps},Average Solve Score {float(f\"{report_card.average_run_score/ report_card.dijkstra_shortest_path_score:.2g}\") } filecount: {filecount} \n",
    "            #       Explore Rewards sum: {float(f\"{np.sum(np.array(current_episode_explore_rewards)):.2g}\")} Exploit Rewards sum: {float(f\"{np.sum(np.array(current_episode_exploit_rewards)):.2g}\")} \n",
    "            #       Success Rate in solving {report_card.success_rate_in_exploitation} Episode: {episode}\"\"\")\n",
    "            with open(f'{folder_name}/doubledqn{filecount}.json', 'w') as file:\n",
    "                file.write(env.to_json_python())\n",
    "            \n",
    "            filecount += 1\n",
    "\n",
    "    # plot_q_histogram(q_value_log)\n",
    "    # plot_losses(losses)\n",
    "    \n",
    "\n",
    "    testing_maze_report_cards = defaultdict(list)\n",
    "    folder_name = f\"../mazeLogs/{timestamp}DoubleDQNMaze/Run{run_id}/Test\"\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "    \n",
    "    \n",
    "    for generation_type in maze_generation_algorithms:\n",
    "        print(\"Starting test with\", generation_type)\n",
    "        filecount = 0\n",
    "        \n",
    "        for episode in range(hyperprm[\"test_episodes\"]):\n",
    "            obs = env.reset_and_regenerate(generation_type)\n",
    "            for mini_episode in range((hyperprm[\"mini_explore_runs_per_episode\"] + hyperprm[\"mini_exploit_runs_per_episode\"])):\n",
    "                obs = env.smart_reset(mini_episode)\n",
    "                for _ in range(hyperprm[\"steps_per_episode\"]):\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                        q_values = model(state_tensor)\n",
    "                        action = maze_library.create_action(int(torch.argmax(q_values).item()), mini_episode)\n",
    "                        obs, _, done, truncated = env.take_action(action) \n",
    "                        \n",
    "                    if done or truncated:\n",
    "                        break\n",
    "            if episode % 20 == 0 and episode != 0:                \n",
    "                with open(f'{folder_name}/{generation_type}{filecount}.json', 'w') as file:\n",
    "                    file.write(env.to_json_python())\n",
    "            filecount += 1        \n",
    "            report_card = maze_library.get_score(env)\n",
    "            testing_maze_report_cards[generation_type].append(report_card.to_json())\n",
    "        \n",
    "        \n",
    "    save_experiment_results_with_pickle ({\n",
    "        \"params\": hyperprm,\n",
    "        \"training_explore_rewards\": all_episode_explore_rewards,\n",
    "        \"training_exploit_rewards\": all_episode_exploit_rewards,\n",
    "        \"training_report\" : report_cards,\n",
    "        \"testing_maze_report_cards\" : dict(testing_maze_report_cards)\n",
    "    }, f\"../mazeLogs/{timestamp}DoubleDQNMaze/Run{run_id}/experiment_results.pkl\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "combo_params = list(generate_param_combinations(hyperparams))\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m_%H-%M\")\n",
    "run_count = 0\n",
    "print(f\"Totals combinations = {len(combo_params)}\")\n",
    "for i, combo in enumerate(combo_params):\n",
    "    for j in range(1):\n",
    "        merged_params = {**shared_params, **combo}\n",
    "        run_single_experiment(merged_params,timestamp, run_id=run_count)\n",
    "        run_count += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
