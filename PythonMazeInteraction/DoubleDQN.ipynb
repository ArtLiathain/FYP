{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DQN Training in Maze Environment (Hyperparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import itertools\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\":5e-6,\n",
    "    \"gamma\": 0.98,\n",
    "    \"epsilon_decay_episodes\": 0.9,\n",
    "    \"batch_size\": 256,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"episodes\": 2000,\n",
    "    \"hidden_layers\": \n",
    "        [256, 256, 256] \n",
    "    ,\n",
    "    \"activation\": \"elu\",\n",
    "    \"attempts_per_episode\": 10,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"steps_per_episode\": 1000,\n",
    "    \"target_update_freq\": 500,\n",
    "    \"random_seed\": 42,\n",
    "    \"maze_width\": 10,\n",
    "    \"maze_height\": 10,\n",
    "    \"maze_imperfections\": 5,\n",
    "    \"input_dim\": 19,\n",
    "    \"n_outputs\": 4,\n",
    "    \"log_every\": 200,\n",
    "    \"q_log_every\": 2000,\n",
    "    \"plot_every\": 1100,\n",
    "    \"train_after\": 0.33333,\n",
    "    \"truncate_after\": 100,\n",
    "    \"target_dqn_update\" : 50\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maze Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "maze_width = 16\n",
    "maze_height = 16\n",
    "\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "env = maze_library.init_environment(maze_width, maze_height, allowed_revisits=hyperparams[\"truncate_after\"])\n",
    "maze_library.make_maze_imperfect(env)\n",
    "\n",
    "input_shape = env.input_shape()\n",
    "n_outputs = env.output_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size,device, hidden_layers=[64, 64, 64], activation=\"elu\", ):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        self.device = device\n",
    "        prev_size = input_size\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            self.model.add_module(f\"fc{i}\", nn.Linear(prev_size, hidden_size))\n",
    "            self.model.add_module(f\"act{i}\", activation_map[activation]())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.model.add_module(\"output\", nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def create_double_dqn_model(hyperprm):\n",
    "    model = DQN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"])\n",
    "    target_model = DQN(input_shape, n_outputs, DEVICE, hyperprm[\"hidden_layers\"], hyperprm[\"activation\"])\n",
    "    target_model.load_state_dict(model.state_dict())  # Initialize target model with the same weights\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=5e-5)\n",
    "    loss_fn = nn.HuberLoss()\n",
    "\n",
    "    return model, target_model, optimizer, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy and Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        self.states = np.empty((capacity, *state_shape), dtype=np.float32)\n",
    "        self.actions = np.empty((capacity,), dtype=np.int64)\n",
    "        self.rewards = np.empty((capacity,), dtype=np.float32)\n",
    "        self.next_states = np.empty((capacity, *state_shape), dtype=np.float32)\n",
    "        self.dones = np.empty((capacity,), dtype=bool)\n",
    "        self.truncateds = np.empty((capacity,), dtype=bool)\n",
    "\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done, truncated):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "        self.truncateds[self.ptr] = truncated\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        batch = dict(\n",
    "            state=self.states[idxs],\n",
    "            action=self.actions[idxs],\n",
    "            reward=self.rewards[idxs],\n",
    "            next_state=self.next_states[idxs],\n",
    "            done=self.dones[idxs],\n",
    "            truncated=self.truncateds[idxs]\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_outputs - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def play_one_step(env, state, epsilon,model, replay_buffer, reward_log, run):\n",
    "    action = epsilon_greedy_policy(state, epsilon, model)\n",
    "    action_obj = maze_library.create_action(action, run)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    reward_log.append(reward)\n",
    "    replay_buffer.append(state, action, reward, next_state, done, truncated)\n",
    "    return next_state, reward, done, truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log):\n",
    "    sample = replay_buffer.sample(hyperprm[\"batch_size\"])\n",
    "    states = torch.tensor(sample[\"state\"], dtype=torch.float32)\n",
    "    actions = torch.tensor(sample[\"action\"], dtype=torch.int64)\n",
    "    rewards = torch.tensor(sample[\"reward\"], dtype=torch.float32)\n",
    "    next_states = torch.tensor(sample[\"next_state\"], dtype=torch.float32)\n",
    "    dones = torch.tensor(sample[\"done\"], dtype=torch.float32)\n",
    "    truncateds = torch.tensor(sample[\"truncated\"], dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model(next_states)\n",
    "        next_actions = next_q_values.argmax(dim=1)\n",
    "        target_q_values = target_model(next_states)\n",
    "        max_next_q = target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "        target_q = rewards + (1 - terminal) * hyperprm[\"gamma\"] * max_next_q\n",
    "\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    q_value_log.extend(Tensor.cpu(selected_q.detach()).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(q_value_log):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(q_value_log)\n",
    "    plt.title(\"Q-Values Over Time\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Q-Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(hyperprm, timestamp,run_id):\n",
    "    all_episode_rewards  = []\n",
    "    reward_log = []\n",
    "    report_cards = []\n",
    "    q_value_log = []\n",
    "    filecount = 0\n",
    "    folder_name = f\"../mazeLogs/{timestamp}DoubleDQNMaze\"\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "    model,target_model, optimizer, loss_fn = create_double_dqn_model(hyperprm)\n",
    "    replay_buffer = ExperienceReplayBuffer(capacity=hyperprm[\"replay_buffer_size\"], state_shape=(input_shape,))\n",
    "    print(f\"Run {run_id}, Episodes: {hyperprm['episodes']}\")\n",
    "\n",
    "    for episode in range(hyperprm[\"episodes\"]):\n",
    "        obs = env.reset_and_regenerate()\n",
    "        cumulative_reward = 0\n",
    "        eps_decay = episode / (hyperprm[\"episodes\"] * hyperprm[\"epsilon_decay_episodes\"])\n",
    "        epsilon = max(hyperprm[\"epsilon_start\"] - eps_decay, hyperprm[\"epsilon_min\"])\n",
    "        for mini_episode in range(hyperprm[\"attempts_per_episode\"]):\n",
    "            env.smart_reset(mini_episode)\n",
    "            for _ in range(hyperprm[\"steps_per_episode\"]):\n",
    "                obs, reward, done, truncated = play_one_step(env, obs, epsilon,model, replay_buffer, reward_log, mini_episode)\n",
    "                cumulative_reward += reward * (mini_episode/hyperprm[\"attempts_per_episode\"])\n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "        if episode > (hyperprm[\"episodes\"] * hyperprm[\"train_after\"]):\n",
    "            training_step(model,target_model ,optimizer, loss_fn, hyperprm, replay_buffer, q_value_log, )\n",
    "        \n",
    "        if episode > hyperprm[\"target_dqn_update\"]:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        score = maze_library.get_score(env)\n",
    "        if episode % hyperprm[\"log_every\"] == 0 and episode != 0:\n",
    "            print(f\"Run {run_id}, Total steps {score.total_steps},Best run {score.best_run_steps} Reward: {np.average(all_episode_rewards[-50:])} Episode: {episode}\")\n",
    "            with open(f'../mazeLogs/{timestamp}DoubleDQNMaze/doubledqn{filecount}.json', 'w') as file:\n",
    "                file.write(env.to_json_python())\n",
    "            filecount += 1\n",
    "        # if episode % hyperprm[\"q_log_every\"] == 0 and episode > 0:\n",
    "        all_episode_rewards.append(cumulative_reward)\n",
    "        report_cards.append(score)\n",
    "    \n",
    "    plot_q_values(q_value_log)\n",
    "\n",
    "    return {\n",
    "        \"params\": hyperprm,\n",
    "        \"rewards\": all_episode_rewards,\n",
    "        \"score\" : report_cards,\n",
    "        \"q_values\": q_value_log,\n",
    "        \"final_model\": model,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "number_of_iterations = 10\n",
    "for i in range(number_of_iterations):\n",
    "    results.append(run_single_experiment(hyperparams,timestamp ,run_id=i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changed_hyperparams(defaults, current):\n",
    "    return {\n",
    "        k: v for k, v in current.items()\n",
    "        if k not in defaults or defaults[k] != v\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [r[\"score\"] for r in results]\n",
    "\n",
    "print(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_nested  = [r[\"score\"] for r in results]\n",
    "steps = [score.total_steps for sublist in steps_nested for score in sublist] \n",
    "def moving_average(data, window_size=3):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Smoothing window size\n",
    "window_size = 15  # You can adjust this window size for more or less smoothing\n",
    "\n",
    "# Determine the number of subplots (rows x columns)\n",
    "n_runs = len(steps_nested)\n",
    "cols = 3  # Number of columns\n",
    "rows = (n_runs // cols) + (n_runs % cols > 0)  # Calculate number of rows required\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "\n",
    "# Flatten axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each run in its own subplot\n",
    "for i, run_scores in enumerate(steps_nested):\n",
    "    steps = [s.total_steps for s in run_scores]\n",
    "    \n",
    "    # Apply moving average smoothing to the data\n",
    "    smoothed_steps = moving_average(steps, window_size)\n",
    "    \n",
    "    # Select the current axis for this subplot\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot the smoothed steps on the corresponding axis\n",
    "    ax.plot(smoothed_steps, label=f\"Run {i+1}\")\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Total Steps\")\n",
    "    ax.set_title(f\"Performance of Run {i+1}\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove any empty subplots if the number of runs isn't a perfect multiple of columns\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_data = []\n",
    "for i, result in enumerate(results):\n",
    "    param_row = {**result[\"params\"]}\n",
    "    param_row[\"run_id\"] = i\n",
    "    param_row[\"mean_reward\"] = sum(result[\"rewards\"]) / len(result[\"rewards\"])\n",
    "    param_row[\"max_reward\"] = max(result[\"rewards\"])\n",
    "    param_row[\"min_reward\"] = min(result[\"rewards\"])\n",
    "    csv_data.append(param_row)\n",
    "\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv(\"results_log2.csv\", index=False)\n",
    "print(\"Saved results to results_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.hist(reward_log, bins=10, edgecolor='black')\n",
    "# plt.title(\"Reward Distribution\")\n",
    "# plt.xlabel(\"Reward\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
