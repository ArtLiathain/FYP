{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(maze_library.init_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = maze_library.init_environment(16,16,allowed_revisits=100)\n",
    "maze_library.make_maze_imperfect(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "input_size = 19\n",
    "output_size = 4\n",
    "model = DQN(input_size, output_size)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.HuberLoss()\n",
    "replay_buffer = deque(maxlen=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, output_size - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_experiences(batch_size):\n",
    "#     sorted_experiences = sorted(replay_buffer, key=lambda exp: abs(exp[2]), reverse=True)\n",
    "#     half_batch = batch_size // 2\n",
    "\n",
    "#     # Ensure there are enough samples to choose from\n",
    "#     num_experiences = len(sorted_experiences)\n",
    "#     if num_experiences < batch_size:\n",
    "#         batch = sorted_experiences\n",
    "#     else:\n",
    "#         high_priority_samples = sorted_experiences[:half_batch]\n",
    "#         random_samples = [sorted_experiences[i] for i in np.random.choice(num_experiences, half_batch, replace=False)]\n",
    "#         batch = high_priority_samples + random_samples\n",
    "    \n",
    "#     return [\n",
    "#         np.array([experience[field_index] for experience in batch], dtype=np.float32)\n",
    "#         for field_index in range(6)\n",
    "#     ]\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), min(batch_size, len(replay_buffer)), replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    return zip(*batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_log = []\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    action_obj = maze_library.create_action(action, 0)\n",
    "    next_state, reward, done, truncated = env.take_action(action_obj)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    reward_log.append(reward)\n",
    "    return next_state, reward, done, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rewards = [] \n",
    "highest_reward = -1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.96\n",
    "q_value_log = []\n",
    "\n",
    "\n",
    "def training_step(batch_size, discount_factor=0.94):\n",
    "    states, actions, rewards, next_states, dones, truncateds = sample_experiences(batch_size)\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "    truncateds = torch.tensor(truncateds, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = model(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        terminal = torch.logical_or(dones.bool(), truncateds.bool()).float()\n",
    "\n",
    "        target_q = rewards + (1 - dones) * discount_factor * max_next_q\n",
    "\n",
    "    q_values = model(states)\n",
    "    selected_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    q_value_log.extend(selected_q.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def plot_q_values():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(q_value_log)\n",
    "    plt.title(\"Q-Values Over Time\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Q-Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "batch_size = 128\n",
    "\n",
    "# In the plotting function:\n",
    "for episode in range(steps):\n",
    "    obs = env.reset()\n",
    "    cumilative_reward = 0\n",
    "    for step in range(steps):\n",
    "        epsilon = max(1 - episode / (steps * 0.9), 0.01)\n",
    "        obs, reward, done, truncated = play_one_step(env, obs, epsilon)\n",
    "        cumilative_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Extra debug information\n",
    "    if episode% 100 == 0 or cumilative_reward > 0:\n",
    "        print(f\"\\rEpisode: {episode}, Steps: {step}, eps: {epsilon:.3f}, reward = {cumilative_reward}\")\n",
    "    rewards.append(cumilative_reward)\n",
    "    \n",
    "    if cumilative_reward > highest_reward:\n",
    "        best_weights = model.state_dict()\n",
    "        highest_reward = cumilative_reward\n",
    "\n",
    "    if episode > 800:\n",
    "        training_step(batch_size)\n",
    "    if episode % 500 == 499:\n",
    "        plot_q_values()\n",
    "        \n",
    "    with open(f'../mazeLogs/solve{episode}.json', 'w') as file:\n",
    "        file.write(env.to_json_python())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 18–10\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(reward_log, bins=1, edgecolor='black')\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
